{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Szkolenie ML/AI 4***\n",
    "### ***Sztuczne sieci neuronowe***\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "<small>Made by Aleksander Kołodziejczyk</small>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: right;\">\n",
    "<small>Zdjęcia i materiały z \"Uczenie maszynowe z użyciem Scikit-Learn i TensorFlow, Autor: Aurelien Geron</small>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sztuczne sieci neuronowe po raz pierwszy zaprezentowane zostały w 1943 roku w artukule *A Logical Calculus of Ideas Immanent in Nervous Activity*. Neurofizjolog Warren McCulloch i matematyk Walter Pitts ukazali uproszczony model działania zespołu neuronów i opisali pierwszą architekturę sztucznej sieci neuronowej (ang. *Artificial neural network*). W tym czasie nastąpiła ekspozja zainteresowaniem sztczną inteligencją, która szybko zgasła."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Jak działa neuron biologiczny?\n",
    "\n",
    "Neurony biologiczne otrzmują z innych neuronów impulsy elektryczne (tzw. sygnały) poprzez synapsy. Gdy komórka nerwowa otrzyma dostateczną ilość sygnałów od innch neuronów w ciągu kilku milisekund to sama zaczyna wysyłać własne sygnały.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_2.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sztuczny neuron ma co najmniej jedno binarne (0 lub 1) wejście i tylko jedno wyjście binarne. Twórcy modelu udowodnili, że za pomocą sieci takich nauronów można rozwiązać dowolne zadanie logiczne.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_3.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron \n",
    "\n",
    "Perceptron stanowi jedną z najprostszych architektur SSN. Został on zaprezentowany w 1957 roku przez Franka Rosenblatta. Jego podstawę stanowi nieco zmodyfikowany sztuczny neuron z wany **jednostką liniową z progiem** (ang. *linear treshold unit - LTU*), dzięki czemu wartościami wejść/wyjść są wartości liczbowe, a nie binarne, a każde połączenie ma przyporządkowaną wagę. Jednostkę LTU wylicza sumę ważoną sygnałów wejściowych $z = w_1 * x_1 + w_2 * x_2 + w_n * x_n$ ($w^T * x$), a następnie wobec tej sumy zostaje użyta funkcja skokowa dając ostateczny wynik.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_4.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/Heaviside.png\" alt=\"drawing\" style=\"width:350px;\"/>\n",
    "</div>\n",
    "Funkcja skokowa Heaviside'a\n",
    "<br>\n",
    "\n",
    "Warto wspomnieć o tym, że pojedyncza jednostka LTU może rozwiązywać zadania prostej klasyfikacji (należy pamiętać o dodaniu cechy obiążenia x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x77c234b238b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5, 5, 1001)\n",
    "y = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    if x[i] < 0: y.append(0)\n",
    "    elif x[i] > 0: y.append(1)\n",
    "    else: y.append(0.5)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(x,y)\n",
    "\n",
    "\n",
    "# print(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron składa się z jednej warstwy jednostek LTU, gdzie każdy neuron jest połączony ze wszystkimy wejściami. Połączenia te są reprezentowane często przez neurony zwane neuronami wejściowymi (ang. *input neuron*). Przekazują one dane wejściowe. Często dodawany jest również dodatkowe obiążenie (x0 = 1), cecha ta jest reprezentowana przez tzw. neuron obciążeniowy. Jego zadaniem jest po prostu przesłanie na wyjście wartości 1.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_5.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie\n",
    "\n",
    "Policz wartości w sieci neuronowej:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/zad_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Funkcja aktywacji dla: \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/sig.png\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozwiązanie na kartce:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/zad_2.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nauczenie perceptronu polega na doborze odpowiednich wag:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_2_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Granica decyzyjna dla każdego neuronu wyjściowego jest jest **liniowa**, dlatego perceptron nie jest w stanie nauczyć się skomplikowanych wzorców. Rosenblatt udowodnił, że jeżeli próbki uczące będą liniowo rozdzielne to algorytm osiągnie zbieżność.\n",
    "\n",
    "Problem z XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights between Input and Hidden layer: [[0.25 0.3 ]\n",
      " [0.15 0.7 ]]\n",
      "\n",
      "Weights between Hidden and Output layer: [[0.5 ]\n",
      " [0.75]]\n",
      "\n",
      "Hidden layer input: [[0.4 1. ]]\n",
      "\n",
      "Hidden layer output: [[0.59868766 0.73105858]]\n",
      "\n",
      "Final layer input: [[0.84763776]]\n",
      "\n",
      "Final layer output: [[0.70007137]]\n",
      "\n",
      "Final output: [[0.70007137]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initializing weights randomly\n",
    "weights_input_hidden = np.array([0.25, 0.3, 0.15, 0.7]).reshape(2, 2)\n",
    "weights_hidden_output = np.array([0.5, 0.75]).reshape(2, 1)\n",
    "\n",
    "# Example inputs (2 features) and target output\n",
    "inputs = np.array([[1, 1]])\n",
    "\n",
    "# Target outputs (one output node)\n",
    "outputs = np.array([[1]])\n",
    "\n",
    "# Displaying results\n",
    "print(f\"\\nWeights between Input and Hidden layer: {weights_input_hidden}\")\n",
    "print(f\"\\nWeights between Hidden and Output layer: {weights_hidden_output}\")\n",
    "\n",
    "# Feedforward propagation\n",
    "hidden_layer_input = np.dot(inputs, weights_input_hidden)\n",
    "print(f\"\\nHidden layer input: {hidden_layer_input}\")\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "print(f\"\\nHidden layer output: {hidden_layer_output}\")\n",
    "\n",
    "final_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "print(f\"\\nFinal layer input: {final_layer_input}\")\n",
    "predicted_output = sigmoid(final_layer_input)\n",
    "print(f\"\\nFinal layer output: {predicted_output}\")\n",
    "\n",
    "print(f\"\\nFinal output: {predicted_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiele ograniczeń perceptronu wyeliminować można poprzez stworzenie wielu warstw perceptronów, tego typu architekturę nazywa się perceptronem wielowarstwowym (ang. *multi-layer perceptron - MLP*) \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_6.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Perceptron wielowarstwowy składa się z: \n",
    "- jednej warstwy wejściowej (przechodnej)\n",
    "- co najmniej jednej warsty ukrytej (składającej się z jednosek/jednostki LTU)\n",
    "- jednej warstwy wyjściowej (składającej się z jednosek/jednostki LTU)\n",
    "\n",
    "<br>\n",
    "\n",
    "Oprócz warstwy wyjściowej każda warstwa może mieć (zwykle ma) neuron obciążający oraz jest w pełni połączona z następną warstwą.\n",
    "\n",
    "Gdy sieć SSN zawiera przynajmniej dwie warstwy ukryte to nosi nazwę głębokiej sieci neuronowej GSN (ang. *deep neural network - DNN*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopiero w 1986 roku D.E. Rumelhart i in. opublikowali artykuł o koncepcji algorytmu wstecznej propagacji (ang. *backpropagation*).\n",
    "\n",
    "Algorytm pobiera każdy przykład uczący do sieci i oblicza wynik wszystkich neuronów w każdej warstwie. Następnie liczony jest błąd na wyjściu sieci ($y^* - y$), a także wpływ każdego neuronu w warstwie na błędny wynik. W dalszej kolejności mierzony jest wpływ neuronów ze wcześniejszej warstwy itd. Taki odwrotny przebieg skutecznie mierzy gradient błędu we wszystkich wagach połączeń porzez propagację tego gradientu w kierunku początku sieci. Ostatnim elementem algorytmu propagacji jest mechanizm gradientu prostego wobec wszystkich wag połączeń w sieci za pomocą zmierzonego wcześniej gradientu błędu.\n",
    "\n",
    "http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html\n",
    "\n",
    "Aby algorytm działał prawidłowo, twórcy musieli zastąpić funkcję skokową funkcją logistyczną (zdjęcia na górze).\n",
    "Zaczęto również testować inne funckje:\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_8.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron wielowarstwowy często jest używany w zadaniach klasyfikacji, gdzie każde wyjście odpowiada innej klasie binarnej. Gdy klasy się wzajemnie wykluczają (np. irysy), warstwa wyjściowa zostaje zmodyfikowana w taki sposób, że funkcje aktywacji zastępowane są współdzieloną funkcją typu softmax. Jeśli sygnał biegnie tylko w jednym kierunku (od wejścia do wyjścia) to taka architektura nazywa się jednokierunkową siecią neuronową (ang. *feedfoward neural network*).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/10_9.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak powinna wyglądać topologia sieci?\n",
    "- topologia sieci już sama w sobie jest hiperparametrem, zbyt uboga - niedouczenie, zbyt skomplikowana - długie czasy obliczeń, większa szansa na przeuczenie.\n",
    "\n",
    "Ile ukrytych warstw?\n",
    "- teoretycznie jedna warstwa ukryta wystarczy (pod warunkiem, że ma wystarczającą ilość neuronów), ale sieci głębokie mają dużą większą efektywność parametrów od sieci płytkich (neurony w warstwach często \"wykrywają\" własności od szczegółu do ogółu). Lepiej więc dodać więcej warstw z mniejszą ilością neuronów.\n",
    "\n",
    "Ile neuronów ma tworzyć warstwę ukrytą?\n",
    "- Warstwa wejściowa jest uzależniona od kształtu danych. Na wejście tyle neuronów ile wejść, a na wyjściu tyle klas czy wartości ile potrzebujemy do rozwiązania problemu. Na przykład w irysach potrzeba 4 neurony wejściowe i 3 wyjściowe (z funkją softmax na końcu). W przypadku warstw ukrytych częstym rozwiązaniem jest podejście lejkowe, gdzie ilość neuronów w wastwach się stopniowo zmniejsza np. podzielić na dwa. Inną techniką jest stosowanie takiej samej ilości neuronów w warstwach ukrytych (ich regulację określa wtedy tylko jeden hiperparametr). \n",
    "- Najprostszym rozwiązaniem jest wybór sieci zawierającej więcej neuronów i warstw niż to potrzebne, a następnie skorzystanie z regulacji wczesnego zatrzymania. Strategia ta nosi nazwę \"rozciągliwych portek\" (ang. *stretch pants*).\n",
    "\n",
    "Jakie funkcje aktywacji użyć?\n",
    "- W większości przypadków używa się modyfikacji sieci ReLU. Jest nieco szybsza od pozostałych funkcji aktywacji, a algorytm gradientu prostego nie zatrzymuje się tak często na wypłaszczeniach. \n",
    "- W przypadku funkcji wyjściowych, klasyfikacja -> softmax, regresja -> nie potrzeba funkcji aktywacji na wyjściu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poblem zanikających/eksplodujących gradientów\n",
    "\n",
    "Wartości gradientów często maleją wraz z przebiegiem algorytmu propagacji wstecznej do niższych warstw sieci. Wskutek tego, wagi nie są znacznie aktualizowane co uniemożliwia sieci osiągnięcia zbieżności. Jest to problem zanikających gradientów (ang. *vanishing gradients*). W niektórych sytuacjach następuje inne zjawisko podczas którego gradienty aktualizowane są w szalonym tempie (ale algorytm staje się rozbieżny), czyli problem eksplodujących gradientów (ang. *exploding gradients*). Ogólnie mówiąc, sieci neuronowe cieprią na syndrom niestabilnych gradientów, poszczególne warstwy mogą uczyć się w diametralnie innym tempie. \n",
    "\n",
    "Problem ten był przez długi czas ignorowany i dopiero w 2010 roku Xavier Glorot i Yoshua Bengio wykryli \"podejrzane\" miejsca.\n",
    "\n",
    "Mianowicie kombinację logistycznej, sigmoidalnej funkcji aktywacji z losowo inicjalizowanymi wagami (rozkład normalny o średniej równej zero i odchyleniem standardowyn równym 1). Podczas przebiegu naprzód wariancja każdej następnej warstwy jest większa, aż do momentu przesycenia funkcji. \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Problem ten należy rozwiązać inicjując wagi następująco (inicjalizacja wag Xaviera lub Glorota):\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_1_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po pewnym czasie oderwano się od połączeń do ludzkiego mózgu i zaczęto stosować inne funkcje aktywacji. Tą która radziła sobie lepiej była ReLU (ang. *rectified linear unit*). Radziła sobie lepiej, ponieważ nie ulegała nasyceniu dla dodatnich wartości (i zmniejszyła czas obliczeń).\n",
    "\n",
    "Funkcja ReLU też nie jest idealna, istnieje coś takiego jak śmierć ReLU (ang. *dying ReLU*). W czasie nauki niektóre neurony giną, co oznacza że na wyjście wysyłają jedynie 0.\n",
    "\n",
    "W celu uniknięcia tego zjawiska zastosowano tzw. przeciekającą fukncję ReLU (ang. *leaky relu*)\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_2.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Kolejną poprawą była funkcja jednostkowo wykładniczo-liniowa (ang. *exponential linear unit - ELU*).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_3.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Czas nauki dla niej był krótszy, ponieważ funkcja jest gładka również w punkcie x = 0\n",
    "\n",
    "Natomiast głównym jej problemem jest dłuższy czas obliczeń ze względu na obecność składowej wykładniczej. Sieć ELU będzie wolniejsza od ReLU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacja wsadowa\n",
    "\n",
    "Sama losowa inicjalizacja wag redukuje problem zanikających/eksplodujących gradientów na początku procesu uczenia, jednak nie zapewnia tej ochrony w późniejszym czasie.\n",
    "\n",
    "W publikacji w 2015 roku Sergey Ioffe i Christian Szegedy zaproponowali technikę zwaną normalizacją wsadową (ang. *batch normalisation* - BN). Jako rozwiązanie problemu rozkładu zmian wejść w każdej warstwie podczas uczenia.\n",
    "\n",
    "Technika ta polega na wstawieniu operacji do modelu tuż przed funkcją aktywacji w każdej warstwie, wyśrodkowaniu i znormalizowaniu danych wejściowych, a następnie przeskalowaniu i przesunięciu wyniku za pomocą dwóch parametrów (jeden odpowiada za skalowanie, a drugi za przesunięcie)\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_3_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Technika ta mocno redukuje problem zanikających gradientów, do tego stopnia, że pod uwagę można zacząć brać tangens hiperboliczny, a nawet funkcję logistyczną. Sieci okazały się również o wiele mniej wrażliwe na losową inicjalizację wag. Wszystko to oczywiście za większą złożoność obliczeniową (ale sama sieć powinna zbiegać szybciej do optimum)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Obcinanie gradientu** - technika polega na ograniczaniu gradientów na etapie propagacji wstecznej w taki sposób, aby nie przekraczały określonego progu. Zdecydowanie bardziej polecana jest normalizacja wsadowa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szybsze optymalizatory\n",
    "\n",
    "Przy bardzo głębokich sieciach uczenie może być bardzo czasochłonne. Kolejnym czynnikiem przyspieszającym naukę jest dobór optymalizatora (coś lepszego niż algorytm gradientu prostego -> jego modyfikacje)\n",
    "\n",
    "<br>\n",
    "\n",
    "**Optymalizacja momentum** (ang. *momentum optimization*) - do algorytmu gradientu prostego dołączany jest człon wektora momentu **m**\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_4_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Przyspieszony spadek wzdłuż gradientu (alogrytm Nesterova)** (ang. *Nesterov momentum optimization* / *Nesterov accelerated gradient* - NAG) - jest to pomiar gradientu funkcji kosztu nie w miejscu lokalnej pozycji, lecz nieco z przodu w kierunku (zwrotu) pędu. Jedyną różnicą w porównaniu do algorytmu optymalizacji momentu to pomiar gradientu za pomocą wyrażenia $\\theta + \\beta m$, a nie $\\theta$\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_6.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**AdaGrad** (ang. *Adaptive Gradient Algorithm*) - algorytm wcześnmiej wykrywający zmianę kierunku i samokorygujący w stronę minimum. Algorytm redukuje współczynnik uczenia, ale dokonuje tego szybciej dla wymiarów o stromym przebiegu funkcji niż dla wymiarów o łagodnym przebiegu. Jest to tak zwany adaptacyjny współczynnik uczenia (ang. *adaptive learning rate*). Uwaga, przez to, że metoda jest adaptacyjna to jest bardziej narażona na utknięcie w minimum lokalnym. \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_6_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_7.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**RMSProp** - Algorytm AdaGrad zwalnia nieco zbyt szybko i kończy działanie, nigdy nie osiągając zbieżności z minimum globalnym. Problem ten rozwiązuje algorytm RMSProp (ciekawostka jest taka, że sam algorytm nigdy nie został opisany w żadnej publikacji, tylko na prezentacji, dlatego badacze odnosząc się do źródła używają zapisu \"slajd 29 na wykładzie 6\"), poprzez gromadzenie wyłącznie gradientów z najbardziej aktualnych przebiegów, a nie wszystkich gradientów od początku nauki. Dokonuje tego poprzez przeprowadzenie rozkładu wykładniczego na pierwszym etapie.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_7_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Optymalizacja Adam** szacowanie adaptacyjnego momentu (ang. *adaptive moment estimation*) - łączy koncepcję optymalizacji momentum i optymalizatora RMSProp, z tego pierwszego używa śledzenie rozkładu wykładniczego średniej wcześniejszych gradientów, a z drugiego śledzenie rozkładu wykładniczego średniej wcześniejszych kwadratów gradientów.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_8_1.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_8_2.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Etapy 3, 4 są techniczne ponieważ wektory **m** i **s** są inicjalizowane z wartością 0, więc na początku będą obciążone w kierunku zera. \n",
    "\n",
    "***UWAGA*** Optymalizacja ADAM obecnie jest uważana za najlepszą, natomiast artykuł z 2017 roku udowodnił, że adaptacyjne techniki optymalizacji mogą prowadzić do rozwiązań które niezbyt sobie radzą z niektórymi zbiorami danych. Problem ten nie występuje np w algorytmie Nesterova.\n",
    "\n",
    "Warto dodać, że wszystkie te techniki oparte są o pochodne pierwszego rzędu (jakobianach), można znaleźć inne algorytmy bazujące na pochodnych drugiego rzędu (hesjanach), ale tutaj złożoność obliczeniowa rośnie na każde wyjście przypada $n^2$ hesjanów (gdzie n to liczba parametrów), w przeciwieństwie do n jakobianów. Przeciętna głeboka sieć neuronowa ma dziesiątki tysiące parametrów, dlatego algorytmy optymalizacji często nie mieszczą się nawet w pamięci maszyny, a te które się mieszczą po prostu są zbyt wolne.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Harmonogramowanie współczynnika uczenia**\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_8.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **odgórny, przedziałowy, stały współczynnik uczenia** (ang. *predetermined piercewise constant learning rate*) - na przykład, początkowo wartość współczynnika uczenia n0 = 0,1 a po 50 epokach zmniejszamy go do wartości n1 = 0,001. Rozwiązanie bywa znakomite, ale często wymaga dodatkowego ustalania wartości oraz momentu zmiany\n",
    "\n",
    "- **Harmonogramowanie wydajnościowe** (ang. *performance scheduling*) - mierzymy błąd walidacji co N przebiegów (jak w metodzie wczesnego zatrzymania) i w momencie gdy wartość błędu przestaje maleć następuje redukcja parametru learning rate o wartość h\n",
    "\n",
    "- **Harmonogramowanie wykładnicze** (ang. *exponential scheduling*) - wyznacza się współczynnik uczenia na podstawie funkcji liczby przebiegów $t = \\mu(t) = \\mu_0 10^{-t/r}$, wartość współczynnika uczenia będzie maleć dziesięciokrotnie co r przebiegów\n",
    "\n",
    "- **Harmonogramowanie potęgowe** (ang. *power scheduling*) - wyznacza się współczynnik uczenia na podstawie funkcji $\\mu(t) = \\mu_0 (1+t/r)^{-c}$. Hiperparametr c ma zwykle wartość 1. Metoda przypomina harmonogramowanie wykładnicze, ale maleje wolniej.\n",
    "\n",
    "Udowodniono, że podczas nauki sieci głębokich za pomocą optymalizacji momentum najlepiej stwierdziły się zarówno harmonogramy wydajnościowe, jak i wykładnicze (z czego te drugie lepiej z powodu łatwiejszej implementacji, strojenia i nieco większej szybkości w uzyskaniu zbieżności). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularyzacja\n",
    "\n",
    "<br>\n",
    "\n",
    "**Wczesne zatrzymanie** - ...\n",
    "\n",
    "<br>\n",
    "\n",
    "**Regularyzacja l1 i l2** - ...\n",
    "\n",
    "<br>\n",
    "\n",
    "**Porzucanie** (ang. *dropout*) - prawdopodobnie najpopularniejsza technika regularyzacji. Używana obecnie nawet w najnowszych sieciach neuronowych. Algorytm ten jest dość trywialny. W poszczególnych przebiegach uczenia neuron (oprócz warstwy wejściowej) może zostać tymczasowo porzucony tzn. całkowicie pominięty w procesie uczenia, ale w każdym z przebiegów może znów być aktywny. Hiperparametr p nosi nazwę współczynnika porzucenia (ang. *dropout rate*). I zazwyczaj ma przyporząądkowaną wartość 50%. Po zakończeniu nauki neurony przestają być porzucane.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_9.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Zakładając, że p = 50% to neuron w trakcie testowania będzie podłączony (średnio) do dwukrotnie większej ilości neuronów niż podczas nauki. Aby zrównoważyć te zjawisko należy pomnożyć wagi wejściowe neuronów przez prawdopodobieństwo zatrzymania (ang. *keep probability*) 1 - p.\n",
    "\n",
    "<br>\n",
    "\n",
    "**max-norm** wagi każdego nauronu są regularyzowane tak, że ||w|| < r, gdzie r to hiperparametr metody max-norm, a ||.|| członem regularyzacji l2. W skrócie ucina się wagi które przekraczają dany próg. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Dogenerowanie danych** (ang. *data augmentation*) - polega na tworzeniu nowych próbek uczących z już istniejących. \n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/11_10.jpg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model rzadki\n",
    "\n",
    "Wcześniej wspomniane algorytmy optymalizacyjne generują modele **gęste**, co oznacza, że większość parametrów przyjmuje niezerowe wartości. Jeśli potrzeba szybkiego modelu lub mamy mało pamięci to lepszym rozwiązaniem jest uzyskanie modelu rzadkiego, czyli takiego w którym parametry np < 0,01 są zmieniane na 0 (występuje więcej zer w parametrach). Inną możliwością jest wprowadzenie silnej regularyzacji l1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podejście praktyczne \n",
    "\n",
    "- Jeśli nie możesz znaleźć optymalnej wartości współczynnika uczenia (model nie osiąga zbieżności zbyt szybko), można spróbować dodać harmonogram uczenia\n",
    "- Jeśli zbiór uczący jest zbyt mały można skorzystać z metody dogenerowania danych\n",
    "- Jeśli należy uzyskać rzadki model należy wprowadzenic regularyzację l1 lub sprowadzać niskie wartości wag do 0\n",
    "- Jeśli potrzeba szybkiego modelu należy zrezyknować z normalizacji wsadowej i np zastąpić funkcję aktywacji ELU przeciekającą funkcją ReLU (oraz wprowadzić model rzadki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splotowe sieci neuronowe (ang. *convolutional neural network*)\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/CNN.jpeg\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "https://poloclub.github.io/cnn-explainer/\n",
    "\n",
    "https://adamharley.com/nn_vis/cnn/3d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rekurencyjne sieci neuronowe (ang. *recurrent neural network*)\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/RNN.png\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: centered;\">\n",
    "    <img src=\"Zdjęcia/Szkolenie_4/RNN_2.png\" alt=\"drawing\" style=\"width:900px;\"/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "https://joshvarty.github.io/VisualizingRNNs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sieci Hopfielda\n",
    "* Maszyny Boltzmanna\n",
    "* Sieci przekonań\n",
    "* Mapy samoorganizujące\n",
    "* Sieci grafowe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
